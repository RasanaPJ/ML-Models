{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data set - Da vinchi code book review comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = pd.read_csv('data\\sentiment_train', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6908</th>\n",
       "      <td>0</td>\n",
       "      <td>Brokeback Mountain is fucking horrible..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6909</th>\n",
       "      <td>0</td>\n",
       "      <td>Then snuck into Brokeback Mountain, which is t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6910</th>\n",
       "      <td>0</td>\n",
       "      <td>, she helped me bobbypin my insanely cool hat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6911</th>\n",
       "      <td>0</td>\n",
       "      <td>My dad's being stupid about brokeback mountain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6912</th>\n",
       "      <td>0</td>\n",
       "      <td>Oh, and Brokeback Mountain is a TERRIBLE movie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6913</th>\n",
       "      <td>0</td>\n",
       "      <td>Brokeback Mountain was boring.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6914</th>\n",
       "      <td>0</td>\n",
       "      <td>So Brokeback Mountain was really depressing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>0</td>\n",
       "      <td>As I sit here, watching the MTV Movie Awards, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok brokeback mountain is such a horrible movie.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>0</td>\n",
       "      <td>Oh, and Brokeback Mountain was a terrible movie.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text\n",
       "6908          0           Brokeback Mountain is fucking horrible..\n",
       "6909          0  Then snuck into Brokeback Mountain, which is t...\n",
       "6910          0  , she helped me bobbypin my insanely cool hat ...\n",
       "6911          0  My dad's being stupid about brokeback mountain...\n",
       "6912          0  Oh, and Brokeback Mountain is a TERRIBLE movie...\n",
       "6913          0                     Brokeback Mountain was boring.\n",
       "6914          0       So Brokeback Mountain was really depressing.\n",
       "6915          0  As I sit here, watching the MTV Movie Awards, ...\n",
       "6916          0    Ok brokeback mountain is such a horrible movie.\n",
       "6917          0   Oh, and Brokeback Mountain was a terrible movie."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit to create dictionary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec.fit(sentiment_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features = count_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yip', 'you', 'young', 'younger', 'your', 'yuck', 'yuh', 'zach', 'zen', 'µª']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_features[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2132"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_vec_df = count_vec.transform(sentiment_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6918, 2132)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_vec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sums = np.sum(sentiment_vec_df.toarray(), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  4, ...,  1, 80,  1], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_frequencies_df = pd.DataFrame({'fearture': x_features, 'count': token_sums})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High frequency words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fearture</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1864</th>\n",
       "      <td>the</td>\n",
       "      <td>3306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>and</td>\n",
       "      <td>2154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>harry</td>\n",
       "      <td>2093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>potter</td>\n",
       "      <td>2093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>code</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>vinci</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>da</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>mountain</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>brokeback</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>love</td>\n",
       "      <td>1624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fearture  count\n",
       "1864        the   3306\n",
       "93          and   2154\n",
       "864       harry   2093\n",
       "1466     potter   2093\n",
       "355        code   2002\n",
       "2009      vinci   2001\n",
       "442          da   2001\n",
       "1272   mountain   2000\n",
       "259   brokeback   2000\n",
       "1171       love   1624"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_frequencies_df.sort_values('count', ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low frequency words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1228"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_frequencies_df[token_frequencies_df['count'] ==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to reduce number of features\n",
    "####  remove stop words\n",
    "####  remove low frequency and hig frequencywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(text.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['third',\n",
       " 'via',\n",
       " 'they',\n",
       " 'that',\n",
       " 'below',\n",
       " 'to',\n",
       " 'although',\n",
       " 'eg',\n",
       " 'mill',\n",
       " 'without',\n",
       " 'indeed',\n",
       " 'among',\n",
       " 'against',\n",
       " 'mine',\n",
       " 'least',\n",
       " 'two',\n",
       " 'will',\n",
       " 'anything',\n",
       " 'next',\n",
       " 'all',\n",
       " 'system',\n",
       " 'being',\n",
       " 'part',\n",
       " 'back',\n",
       " 'almost',\n",
       " 'fifteen',\n",
       " 'front',\n",
       " 'latterly',\n",
       " 'should',\n",
       " 'were',\n",
       " 'others',\n",
       " 'down',\n",
       " 'either',\n",
       " 'interest',\n",
       " 'elsewhere',\n",
       " 'them',\n",
       " 'enough',\n",
       " 'first',\n",
       " 'has',\n",
       " 'fill',\n",
       " 'whose',\n",
       " 'already',\n",
       " 'become',\n",
       " 'may',\n",
       " 'empty',\n",
       " 'other',\n",
       " 'anywhere',\n",
       " 'except',\n",
       " 'toward',\n",
       " 'between',\n",
       " 'eight',\n",
       " 'ourselves',\n",
       " 'over',\n",
       " 'what',\n",
       " 'sometimes',\n",
       " 'themselves',\n",
       " 'also',\n",
       " 'during',\n",
       " 'formerly',\n",
       " 'co',\n",
       " 'find',\n",
       " 'call',\n",
       " 'than',\n",
       " 'these',\n",
       " 'us',\n",
       " 'five',\n",
       " 'thereby',\n",
       " 'serious',\n",
       " 'further',\n",
       " 'within',\n",
       " 'thru',\n",
       " 'seeming',\n",
       " 'though',\n",
       " 'give',\n",
       " 'nor',\n",
       " 'nothing',\n",
       " 'ten',\n",
       " 'ever',\n",
       " 'bottom',\n",
       " 'former',\n",
       " 'done',\n",
       " 'i',\n",
       " 'seemed',\n",
       " 'am',\n",
       " 'under',\n",
       " 'me',\n",
       " 'our',\n",
       " 'mostly',\n",
       " 'six',\n",
       " 'whereby',\n",
       " 'namely',\n",
       " 'nowhere',\n",
       " 'becoming',\n",
       " 'hasnt',\n",
       " 'go',\n",
       " 'couldnt',\n",
       " 'fifty',\n",
       " 'behind',\n",
       " 'the',\n",
       " 'there',\n",
       " 'been',\n",
       " 'even',\n",
       " 'neither',\n",
       " 'such',\n",
       " 'twenty',\n",
       " 'three',\n",
       " 'but',\n",
       " 'above',\n",
       " 'ours',\n",
       " 'across',\n",
       " 'through',\n",
       " 'in',\n",
       " 'see',\n",
       " 'every',\n",
       " 'seem',\n",
       " 'for',\n",
       " 'thus',\n",
       " 'out',\n",
       " 'with',\n",
       " 'anyhow',\n",
       " 'something',\n",
       " 'nevertheless',\n",
       " 'yourselves',\n",
       " 'most',\n",
       " 'therefore',\n",
       " 'whole',\n",
       " 'we',\n",
       " 'or',\n",
       " 'before',\n",
       " 'top',\n",
       " 'while',\n",
       " 'put',\n",
       " 'could',\n",
       " 'seems',\n",
       " 'whatever',\n",
       " 'any',\n",
       " 'himself',\n",
       " 'might',\n",
       " 'along',\n",
       " 'whenever',\n",
       " 'ie',\n",
       " 'whereupon',\n",
       " 'however',\n",
       " 'four',\n",
       " 'it',\n",
       " 'anyway',\n",
       " 'thick',\n",
       " 'nobody',\n",
       " 'once',\n",
       " 'together',\n",
       " 'well',\n",
       " 'whereafter',\n",
       " 'none',\n",
       " 'whereas',\n",
       " 'perhaps',\n",
       " 'eleven',\n",
       " 'how',\n",
       " 'would',\n",
       " 'cry',\n",
       " 'about',\n",
       " 'an',\n",
       " 'amongst',\n",
       " 'until',\n",
       " 'a',\n",
       " 'hence',\n",
       " 'amount',\n",
       " 'move',\n",
       " 'somewhere',\n",
       " 'detail',\n",
       " 'few',\n",
       " 'whence',\n",
       " 'around',\n",
       " 'was',\n",
       " 'moreover',\n",
       " 'only',\n",
       " 'where',\n",
       " 'another',\n",
       " 'hereafter',\n",
       " 'thereafter',\n",
       " 'ltd',\n",
       " 'beforehand',\n",
       " 'itself',\n",
       " 'do',\n",
       " 'herein',\n",
       " 'much',\n",
       " 'sometime',\n",
       " 'last',\n",
       " 'off',\n",
       " 'sincere',\n",
       " 'many',\n",
       " 'inc',\n",
       " 'he',\n",
       " 'towards',\n",
       " 'thence',\n",
       " 'here',\n",
       " 'somehow',\n",
       " 'very',\n",
       " 'why',\n",
       " 'everything',\n",
       " 'now',\n",
       " 're',\n",
       " 'which',\n",
       " 'wherever',\n",
       " 'noone',\n",
       " 'some',\n",
       " 'always',\n",
       " 'no',\n",
       " 'into',\n",
       " 'as',\n",
       " 'from',\n",
       " 'is',\n",
       " 'rather',\n",
       " 'after',\n",
       " 'besides',\n",
       " 'keep',\n",
       " 'so',\n",
       " 'thereupon',\n",
       " 'con',\n",
       " 'same',\n",
       " 'hers',\n",
       " 'more',\n",
       " 'not',\n",
       " 'etc',\n",
       " 'else',\n",
       " 'take',\n",
       " 'sixty',\n",
       " 'you',\n",
       " 'full',\n",
       " 'are',\n",
       " 'everyone',\n",
       " 'upon',\n",
       " 'cant',\n",
       " 'whoever',\n",
       " 'both',\n",
       " 'if',\n",
       " 'twelve',\n",
       " 'please',\n",
       " 'due',\n",
       " 'throughout',\n",
       " 'made',\n",
       " 'therein',\n",
       " 'de',\n",
       " 'yet',\n",
       " 'less',\n",
       " 'can',\n",
       " 'by',\n",
       " 'those',\n",
       " 'at',\n",
       " 'one',\n",
       " 'when',\n",
       " 'her',\n",
       " 'afterwards',\n",
       " 'have',\n",
       " 'own',\n",
       " 'since',\n",
       " 'nine',\n",
       " 'must',\n",
       " 'bill',\n",
       " 'onto',\n",
       " 'yours',\n",
       " 'amoungst',\n",
       " 'meanwhile',\n",
       " 'had',\n",
       " 'because',\n",
       " 'anyone',\n",
       " 'my',\n",
       " 'each',\n",
       " 'of',\n",
       " 'yourself',\n",
       " 'on',\n",
       " 'again',\n",
       " 'up',\n",
       " 'his',\n",
       " 'alone',\n",
       " 'still',\n",
       " 'fire',\n",
       " 'found',\n",
       " 'hereby',\n",
       " 'hundred',\n",
       " 'describe',\n",
       " 'latter',\n",
       " 'myself',\n",
       " 'cannot',\n",
       " 'name',\n",
       " 'who',\n",
       " 'she',\n",
       " 'someone',\n",
       " 'get',\n",
       " 'and',\n",
       " 'several',\n",
       " 'too',\n",
       " 'be',\n",
       " 'became',\n",
       " 'forty',\n",
       " 'often',\n",
       " 'side',\n",
       " 'herself',\n",
       " 'then',\n",
       " 'this',\n",
       " 'beside',\n",
       " 'beyond',\n",
       " 'becomes',\n",
       " 'whom',\n",
       " 'un',\n",
       " 'wherein',\n",
       " 'their',\n",
       " 'per',\n",
       " 'never',\n",
       " 'otherwise',\n",
       " 'thin',\n",
       " 'everywhere',\n",
       " 'its',\n",
       " 'whither',\n",
       " 'hereupon',\n",
       " 'whether',\n",
       " 'your',\n",
       " 'show',\n",
       " 'him']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding our own set of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords +['movies']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec_v2 = CountVectorizer(stop_words = stopwords, \n",
    "                               min_df = 3,\n",
    "                              max_df = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=0.7, max_features=None, min_df=3,\n",
       "                ngram_range=(1, 1), preprocessor=None,\n",
       "                stop_words=['third', 'via', 'they', 'that', 'below', 'to',\n",
       "                            'although', 'eg', 'mill', 'without', 'indeed',\n",
       "                            'among', 'against', 'mine', 'least', 'two', 'will',\n",
       "                            'anything', 'next', 'all', 'system', 'being',\n",
       "                            'part', 'back', 'almost', 'fifteen', 'front',\n",
       "                            'latterly', 'should', 'were', ...],\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec_v2.fit(sentiment_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features =count_vec_v2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                     text\n",
       "0          1  The Da Vinci Code book is just awesome."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_vec_df = count_vec_v2.transform(sentiment_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_features.index('awesome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_vec_df.toarray()[0:1, 15:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40550"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_vec_df.getnnz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013474760162561102"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_vec_df.getnnz()/(6918*435)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_vec_ds =pd.DataFrame(sentiment_vec_df.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 435 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9   ...   425  426  427  428  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "2    0    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "3    0    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "4    0    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "\n",
       "   429  430  431  432  433  434  \n",
       "0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 435 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_vec_ds[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_vec_ds.columns =x_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>17</th>\n",
       "      <th>6th</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absurd</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>aching</th>\n",
       "      <th>acne</th>\n",
       "      <th>action</th>\n",
       "      <th>actually</th>\n",
       "      <th>...</th>\n",
       "      <th>world</th>\n",
       "      <th>worth</th>\n",
       "      <th>worthless</th>\n",
       "      <th>wotshisface</th>\n",
       "      <th>write</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>zen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 435 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  17  6th  absolutely  absurd  acceptable  aching  acne  action  \\\n",
       "0   0   0    0           0       0           0       0     0       0   \n",
       "1   0   0    0           0       0           0       0     0       0   \n",
       "2   0   0    0           0       0           0       0     0       0   \n",
       "3   0   0    0           0       0           0       0     0       0   \n",
       "4   0   0    0           0       0           0       0     0       0   \n",
       "\n",
       "   actually ...   world  worth  worthless  wotshisface  write  wrong  yeah  \\\n",
       "0         0 ...       0      0          0            0      0      0     0   \n",
       "1         0 ...       0      0          0            0      0      0     0   \n",
       "2         0 ...       0      0          0            0      0      0     0   \n",
       "3         0 ...       0      0          0            0      0      0     0   \n",
       "4         0 ...       0      0          0            0      0      0     0   \n",
       "\n",
       "   year  yes  zen  \n",
       "0     0    0    0  \n",
       "1     0    0    0  \n",
       "2     0    0    0  \n",
       "3     0    0    0  \n",
       "4     0    0    0  \n",
       "\n",
       "[5 rows x 435 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_vec_ds[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_vec_ds['sentiment'] = sentiment_df.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22a4b758630>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sn.barplot(x = 'sentiment',\n",
    "           y = 'awesome',           \n",
    "           data = sentiment_vec_ds,\n",
    "           estimator = sum\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sentiment_vec_ds[x_features],\n",
    "                                  sentiment_vec_ds['sentiment'],\n",
    "                                  train_size = 0.8,\n",
    "                                  random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_vl = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_vl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb_vl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_vl = confusion_matrix(y_test, y_pred, [1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[801,   3],\n",
       "       [ 23, 557]], dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       580\n",
      "           1       0.97      1.00      0.98       804\n",
      "\n",
      "    accuracy                           0.98      1384\n",
      "   macro avg       0.98      0.98      0.98      1384\n",
      "weighted avg       0.98      0.98      0.98      1384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9720873786407767"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "801/(23+801)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_v1 =RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {'n_estimators': [100],\n",
    "            'criterion' : ['gini'],\n",
    "            'max_depth' : [10,15],\n",
    "            'max_features': [0.1, 0.2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_v1 = GridSearchCV(rf_v1,\n",
    "                      param_grid = rf_params,\n",
    "                      cv = 10,\n",
    "                      scoring = 'roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators='warn', n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'criterion': ['gini'], 'max_depth': [10, 15],\n",
       "                         'max_features': [0.1, 0.2], 'n_estimators': [100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_v1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 15,\n",
       " 'max_features': 0.2,\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_v1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.99806, std: 0.00103, params: {'criterion': 'gini', 'max_depth': 10, 'max_features': 0.1, 'n_estimators': 100},\n",
       " mean: 0.99860, std: 0.00060, params: {'criterion': 'gini', 'max_depth': 10, 'max_features': 0.2, 'n_estimators': 100},\n",
       " mean: 0.99892, std: 0.00065, params: {'criterion': 'gini', 'max_depth': 15, 'max_features': 0.1, 'n_estimators': 100},\n",
       " mean: 0.99934, std: 0.00041, params: {'criterion': 'gini', 'max_depth': 15, 'max_features': 0.2, 'n_estimators': 100}]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_v1.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=15, max_features=0.2, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_v1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_final = grid_v1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp_df = pd.DataFrame({'feature' : x_features,\n",
    "                              'importance': rf_final.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>love</td>\n",
       "      <td>0.205332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>awesome</td>\n",
       "      <td>0.133214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>sucked</td>\n",
       "      <td>0.074402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>sucks</td>\n",
       "      <td>0.073639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>hate</td>\n",
       "      <td>0.054732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>like</td>\n",
       "      <td>0.047970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>loved</td>\n",
       "      <td>0.044988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>mission</td>\n",
       "      <td>0.043628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>impossible</td>\n",
       "      <td>0.035395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>suck</td>\n",
       "      <td>0.031966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>0.031155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>stupid</td>\n",
       "      <td>0.026509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>depressing</td>\n",
       "      <td>0.014184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>movie</td>\n",
       "      <td>0.013872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>horrible</td>\n",
       "      <td>0.009310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>brokeback</td>\n",
       "      <td>0.008268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>terrible</td>\n",
       "      <td>0.008264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>boring</td>\n",
       "      <td>0.007589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>man</td>\n",
       "      <td>0.007016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>good</td>\n",
       "      <td>0.006652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feature  importance\n",
       "245        love    0.205332\n",
       "19      awesome    0.133214\n",
       "365      sucked    0.074402\n",
       "367       sucks    0.073639\n",
       "173        hate    0.054732\n",
       "230        like    0.047970\n",
       "246       loved    0.044988\n",
       "263     mission    0.043628\n",
       "198  impossible    0.035395\n",
       "364        suck    0.031966\n",
       "24    beautiful    0.031155\n",
       "363      stupid    0.026509\n",
       "91   depressing    0.014184\n",
       "269       movie    0.013872\n",
       "193    horrible    0.009310\n",
       "40    brokeback    0.008268\n",
       "377    terrible    0.008264\n",
       "39       boring    0.007589\n",
       "257         man    0.007016\n",
       "158        good    0.006652"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imp_df.sort_values('importance', ascending = False)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Stemming and lemmatization and TF-Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball  import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemmed_words( doc) :\n",
    "    stemmed_words = [stemmer.stem(x) for x in analyzer(doc)]    \n",
    "    final_words = [word for word in stemmed_words if word not in stopwords]\n",
    "    return final_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', 'play', 'love']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stemmed_words(\"the is loved playing lovely\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid_vec = TfidfVectorizer(tokenizer=get_stemmed_words,ngram_range=(1,2), max_df=0.8, min_df=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.8, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function get_stemmed_words at 0x000001CC88D99AE8>,\n",
       "        use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid_vec.fit(sentiment_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid_df = tfid_vec.transform(sentiment_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features = tfid_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_features.index('awesom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid_df.toarray()[0:1, 15:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
